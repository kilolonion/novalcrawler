#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨å°è¯´çˆ¬è™« - å‘½ä»¤è¡Œæ¥å£
Universal Novel Crawler - Command Line Interface
"""

import argparse
import getpass
import sys
from urllib.parse import urlparse

from .crawler import UniversalNovelCrawler
from .login_manager import LoginManager, LoginConfig
from .site_detector import SiteDetector
from .utils import safe_print, print_banner

try:
    from rich.console import Console
    console = Console()
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False
    console = None

def create_cli_parser() -> argparse.ArgumentParser:
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description='ğŸ•·ï¸  é€šç”¨å°è¯´çˆ¬è™« v1.9 - æ™ºèƒ½æµè§ˆå™¨ç™»å½• & ç« èŠ‚åˆå¹¶',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
âš–ï¸  é‡è¦å…è´£å£°æ˜ IMPORTANT DISCLAIMER:
ğŸ›‘ æœ¬è½¯ä»¶ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œä¸¥ç¦ç”¨äºä»»ä½•è¿æ³•è¿è§„æ´»åŠ¨
ğŸ›‘ This software is for learning and communication ONLY
â€¢ ä½¿ç”¨è€…éœ€éµå®ˆæ‰€åœ¨åœ°åŒºæ³•å¾‹æ³•è§„åŠç½‘ç«™æœåŠ¡æ¡æ¬¾
â€¢ è½¯ä»¶ä½œè€…ä¸æ‰¿æ‹…å› ä½¿ç”¨æœ¬è½¯ä»¶äº§ç”Ÿçš„ä»»ä½•æ³•å¾‹è´£ä»»
â€¢ æ‰€æœ‰åæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ‹…ï¼Œä¸ä½œè€…æ— å…³
â€¢ ç¦æ­¢çˆ¬å–æ”¿åºœ/å†›äº‹/æ•æ„Ÿæœºæ„ç½‘ç«™
â€¢ ä½¿ç”¨æœ¬è½¯ä»¶å³è¡¨ç¤ºåŒæ„æ­¤å…è´£å£°æ˜

ä½¿ç”¨ç¤ºä¾‹:
  universal_novel_crawler.py                                    # äº¤äº’æ¨¡å¼
  universal_novel_crawler.py -u https://example.com/novel/      # æŒ‡å®šURL
  universal_novel_crawler.py -u URL -r 1-100 -t 5              # ä¸‹è½½1-100ç« ï¼Œ5çº¿ç¨‹
  universal_novel_crawler.py -u URL --login browser             # ä½¿ç”¨æµè§ˆå™¨ç™»å½•
  universal_novel_crawler.py -u URL --no-login                  # æ— éœ€ç™»å½•æ¨¡å¼
  universal_novel_crawler.py -u URL --merge                     # ä¸‹è½½åè‡ªåŠ¨åˆå¹¶
  universal_novel_crawler.py --list-sites                       # æ˜¾ç¤ºæ”¯æŒçš„ç½‘ç«™

ç« èŠ‚èŒƒå›´æ ¼å¼:
  100-200    ä¸‹è½½ç¬¬100åˆ°200ç« 
  50:        ä»ç¬¬50ç« å¼€å§‹ä¸‹è½½
  :100       ä¸‹è½½å‰100ç« 
  100+       ä»ç¬¬100ç« å¼€å§‹åˆ°ç»“å°¾
  150        åªä¸‹è½½ç¬¬150ç« 
        """
    )
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('-u', '--url', help='å°è¯´ç›®å½•é¡µé¢URL')
    parser.add_argument('-r', '--range', help='ç« èŠ‚èŒƒå›´ (ä¾‹: 1-100, 50:, :100, 100+, 150)')
    parser.add_argument('-t', '--threads', type=int, default=3, 
                       help='å¹¶å‘çº¿ç¨‹æ•° (1-10, é»˜è®¤: 3)')
    parser.add_argument('-o', '--output', help='è¾“å‡ºç›®å½• (é»˜è®¤: novels_ç½‘ç«™å)')
    
    # ç™»å½•ç›¸å…³
    login_group = parser.add_mutually_exclusive_group()
    login_group.add_argument('--login', choices=['browser', 'cookies', 'password', 'auto'],
                            help='ç™»å½•æ–¹å¼: browser(æµè§ˆå™¨), cookies(Cookieæå–), password(ç”¨æˆ·åå¯†ç ), auto(è‡ªåŠ¨Cookie)')
    login_group.add_argument('--no-login', action='store_true', help='æ— éœ€ç™»å½•æ¨¡å¼')
    
    # ç™»å½•å‚æ•°
    parser.add_argument('--username', help='ç™»å½•ç”¨æˆ·å')
    parser.add_argument('--password', help='ç™»å½•å¯†ç ')
    parser.add_argument('--cookies', help='Cookieå­—ç¬¦ä¸²')
    parser.add_argument('--browser', choices=['chrome', 'edge', 'auto'], default='auto',
                       help='æµè§ˆå™¨é€‰æ‹© (é»˜è®¤: auto)')
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument('--merge', action='store_true',
                       help='ä¸‹è½½å®Œæˆåè‡ªåŠ¨åˆå¹¶æ‰€æœ‰ç« èŠ‚ä¸ºä¸€ä¸ªtxtæ–‡ä»¶')
    parser.add_argument('--skip-robots', action='store_true',
                       help='è·³è¿‡ robots.txt éªŒè¯ (ä¸æ¨è)')
    parser.add_argument('--skip-check-files', action='store_true',
                       help='è·³è¿‡å·²ä¸‹è½½æ–‡ä»¶æ£€æŸ¥ (ç¦ç”¨æ–­ç‚¹ç»­ä¼ )')
    parser.add_argument('--list-sites', action='store_true',
                       help='æ˜¾ç¤ºæ”¯æŒçš„ç½‘ç«™åˆ—è¡¨')
    parser.add_argument('--debug', action='store_true',
                       help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--version', action='version', version='%(prog)s v1.9')
    # è·³è¿‡å…è´£å£°æ˜
    parser.add_argument('-y', '--yes', action='store_true',
                       help='è‡ªåŠ¨ç¡®è®¤å…è´£å£°æ˜(è·³è¿‡æŒ‰å›è½¦)')
    
    return parser

def show_supported_sites():
    """æ˜¾ç¤ºæ”¯æŒçš„ç½‘ç«™åˆ—è¡¨"""
    detector = SiteDetector()
    
    print("ğŸŒ æ”¯æŒçš„ç½‘ç«™åˆ—è¡¨:")
    print("=" * 50)
    
    for site_key, config in detector.site_configs.items():
        print(f"ğŸ“š {config.name}")
        print(f"   åŸŸå: {site_key}")
        print(f"   é€‰æ‹©å™¨: {len(config.catalog_selectors)} ä¸ªç›®å½•é€‰æ‹©å™¨")
        print()
    
    print("ğŸ’¡ æç¤º: é™¤ä»¥ä¸Šç½‘ç«™å¤–ï¼Œæœ¬å·¥å…·è¿˜æ”¯æŒå¤§å¤šæ•°å°è¯´ç½‘ç«™çš„é€šç”¨è§£æ")
    
def setup_login_from_args(login_manager: LoginManager, args, site_url: str):
    """æ ¹æ®å‘½ä»¤è¡Œå‚æ•°è®¾ç½®ç™»å½•é…ç½®"""
    if args.no_login:
        login_manager.login_config.mode = 'none'
        safe_print("âœ… è®¾ç½®ä¸ºæ— éœ€ç™»å½•æ¨¡å¼")
        
    elif args.login:
        if args.login == 'browser':
            login_manager.login_config.mode = 'browser_login'
            login_manager.login_config.browser = args.browser
            safe_print(f"âœ… è®¾ç½®ä¸ºæµè§ˆå™¨ç™»å½•æ¨¡å¼ ({args.browser})")
            
        elif args.login == 'cookies':
            login_manager.login_config.mode = 'browser_cookies'
            login_manager.login_config.browser = args.browser
            safe_print(f"âœ… è®¾ç½®ä¸ºCookieè‡ªåŠ¨æå–æ¨¡å¼ ({args.browser})")
            
        elif args.login == 'password':
            login_manager.login_config.mode = 'credentials'
            if args.username and args.password:
                login_manager.login_config.username = args.username
                login_manager.login_config.password = args.password
            else:
                login_manager.login_config.username = input("è¯·è¾“å…¥ç”¨æˆ·å: ").strip()
                login_manager.login_config.password = getpass.getpass("è¯·è¾“å…¥å¯†ç : ")
            
            # è®¾ç½®ç™»å½•URL
            parsed = urlparse(site_url)
            base_url = f"{parsed.scheme}://{parsed.netloc}"
            login_manager.login_config.login_url = base_url + "/login"
            safe_print("âœ… è®¾ç½®ä¸ºç”¨æˆ·åå¯†ç ç™»å½•æ¨¡å¼")
            
        elif args.login == 'auto':
            login_manager.login_config.mode = 'browser_cookies'
            login_manager.login_config.browser = args.browser
            safe_print(f"âœ… è®¾ç½®ä¸ºè‡ªåŠ¨Cookieæå–æ¨¡å¼ ({args.browser})")
    else:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šç™»å½•å‚æ•°ï¼Œä½¿ç”¨äº¤äº’æ¨¡å¼
        login_manager.get_login_config(site_url)

def ask_continue() -> bool:
    """è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­çˆ¬å–å…¶ä»–å°è¯´"""
    if RICH_AVAILABLE and console:
        from rich.prompt import Confirm
        return Confirm.ask(
            "\nğŸ”„ [bold green]æ˜¯å¦ç»§ç»­çˆ¬å–å…¶ä»–å°è¯´ï¼Ÿ[/bold green]", 
            default=True
        )
    else:
        print()
        user_input = input("ğŸ”„ æ˜¯å¦ç»§ç»­çˆ¬å–å…¶ä»–å°è¯´ï¼Ÿ (y/n, é»˜è®¤y): ").strip().lower()
        return user_input in ['', 'y', 'yes']

def run_single_crawl(args) -> bool:
    """æ‰§è¡Œå•æ¬¡çˆ¬å–ä»»åŠ¡ï¼Œè¿”å›æ˜¯å¦æˆåŠŸ"""
    # è·å–URL
    if args.url:
        catalog_url = args.url
        safe_print(f"ğŸ“– ç›®æ ‡URL: [blue]{catalog_url}[/blue]", style="cyan")
    else:
        print("\n" + "="*60)
        safe_print("ğŸ†• å¼€å§‹æ–°çš„çˆ¬å–ä»»åŠ¡", style="bold green")
        print("="*60)
        if RICH_AVAILABLE and console:
            catalog_url = console.input("ğŸ“– è¯·è¾“å…¥å°è¯´ç›®å½•é¡µURL: ").strip()
        else:
            catalog_url = input("ğŸ“– è¯·è¾“å…¥å°è¯´ç›®å½•é¡µURL: ").strip()
    
    if not catalog_url:
        safe_print("âŒ URLä¸èƒ½ä¸ºç©º", style="bold red")
        return False
    
    # è·å–çº¿ç¨‹æ•°
    max_workers = args.threads
    if not args.url:  # äº¤äº’æ¨¡å¼æ—¶è¯¢é—®
        if RICH_AVAILABLE and console:
            workers_input = console.input(f"âš¡ è¯·è¾“å…¥å¹¶å‘çº¿ç¨‹æ•° [dim](1-10, é»˜è®¤{max_workers})[/dim]: ").strip()
        else:
            workers_input = input(f"âš¡ è¯·è¾“å…¥å¹¶å‘çº¿ç¨‹æ•° (1-10, é»˜è®¤{max_workers}): ").strip()
        if workers_input.isdigit():
            max_workers = max(1, min(10, int(workers_input)))
    
    # è·å–ç« èŠ‚èŒƒå›´
    range_input = args.range
    if not args.url:  # äº¤äº’æ¨¡å¼æ—¶è¯¢é—®
        if RICH_AVAILABLE and console:
            from rich.panel import Panel
            from rich import box
            # ç¾åŒ–çš„èŒƒå›´é€‰æ‹©æç¤º
            range_help = """
ğŸ“š ç« èŠ‚èŒƒå›´æ ¼å¼:
  â€¢ [cyan]100-200[/cyan]  : ä¸‹è½½ç¬¬100åˆ°200ç« 
  â€¢ [cyan]50:[/cyan]      : ä»ç¬¬50ç« å¼€å§‹ä¸‹è½½
  â€¢ [cyan]:100[/cyan]     : ä¸‹è½½å‰100ç« 
  â€¢ [cyan]100+[/cyan]     : ä»ç¬¬100ç« å¼€å§‹åˆ°ç»“å°¾
  â€¢ [cyan]150[/cyan]      : åªä¸‹è½½ç¬¬150ç« 
            """.strip()
            
            panel = Panel(
                range_help,
                title="ğŸ“‹ ç« èŠ‚èŒƒå›´é€‰æ‹© (ç•™ç©ºä¸‹è½½å…¨éƒ¨)",
                border_style="yellow",
                box=box.ROUNDED
            )
            console.print(panel)
            range_input = console.input("è¯·è¾“å…¥ç« èŠ‚èŒƒå›´: ").strip()
        else:
            print("\nğŸ“š ç« èŠ‚èŒƒå›´é€‰æ‹© (ç•™ç©ºä¸‹è½½å…¨éƒ¨):")
            print("  æ ¼å¼ç¤ºä¾‹:")
            print("  â€¢ 100-200  : ä¸‹è½½ç¬¬100åˆ°200ç« ")
            print("  â€¢ 50:      : ä»ç¬¬50ç« å¼€å§‹ä¸‹è½½")
            print("  â€¢ :100     : ä¸‹è½½å‰100ç« ")
            print("  â€¢ 100+     : ä»ç¬¬100ç« å¼€å§‹åˆ°ç»“å°¾")
            print("  â€¢ 150      : åªä¸‹è½½ç¬¬150ç« ")
            range_input = input("è¯·è¾“å…¥ç« èŠ‚èŒƒå›´: ").strip()
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    login_config = LoginConfig()
    login_manager = LoginManager(login_config)
    detector = SiteDetector()
    crawler = UniversalNovelCrawler(login_manager, detector)
    
    # robots.txt æ£€æŸ¥å¯é€‰ï¼ˆå¿…é¡»åœ¨é¦–æ¬¡ç½‘ç»œè¯·æ±‚å‰è®¾ç½®ï¼‰
    if args.skip_robots:
        setattr(crawler, 'skip_robots', True)
    
    # æ–‡ä»¶æ£€æŸ¥å¯é€‰ï¼ˆæ–­ç‚¹ç»­ä¼ åŠŸèƒ½ï¼‰
    if args.skip_check_files:
        setattr(crawler, 'skip_check_files', True)
    
    # è®¾ç½®ç™»å½•é…ç½®
    if args.url and (args.login or args.no_login):
        # CLIæ¨¡å¼ï¼šæ ¹æ®å‚æ•°è®¾ç½®ç™»å½•
        setup_login_from_args(login_manager, args, catalog_url)
    else:
        # äº¤äº’æ¨¡å¼ï¼šè¯¢é—®ç”¨æˆ·
        login_manager.get_login_config(catalog_url)
    
    # æ‰§è¡Œç™»å½•
    crawler.session = login_manager.ensure_login(catalog_url)
    # åªæœ‰éæ— éœ€ç™»å½•æ¨¡å¼æ‰éªŒè¯ç™»å½•çŠ¶æ€
    if login_manager.login_config.mode != 'none' and not login_manager.verify_login():
        safe_print("âŒ ç™»å½•å¤±è´¥ï¼Œç¨‹åºé€€å‡º", style="bold red")
        return False
    
    # è¯¢é—®æ˜¯å¦å¯ç”¨robots.txtæ£€æŸ¥ï¼ˆä»…åœ¨äº¤äº’æ¨¡å¼ä¸”æœªæŒ‡å®š--skip-robotsæ—¶ï¼‰
    if not args.url and not args.skip_robots:  # äº¤äº’æ¨¡å¼
        if RICH_AVAILABLE and console:
            from rich.prompt import Confirm
            enable_robots = Confirm.ask(
                "ğŸ¤– [yellow]æ˜¯å¦å¯ç”¨ robots.txt æ£€æŸ¥ï¼Ÿ[/yellow]", 
                default=True
            )
        else:
            user_input = input("ğŸ¤– æ˜¯å¦å¯ç”¨ robots.txt æ£€æŸ¥ï¼Ÿ (y/n, é»˜è®¤y): ").strip().lower()
            enable_robots = user_input in ['', 'y', 'yes']
        
        if not enable_robots:
            setattr(crawler, 'skip_robots', True)
            safe_print("âš ï¸ å·²ç¦ç”¨ robots.txt æ£€æŸ¥", style="yellow")
        else:
            safe_print("âœ… å·²å¯ç”¨ robots.txt æ£€æŸ¥", style="green")
    
    # è·å–ç« èŠ‚åˆ—è¡¨
    safe_print("ğŸ” æ­£åœ¨è·å–ç« èŠ‚åˆ—è¡¨...", style="yellow")
    chapters = crawler.get_chapter_list(catalog_url)
    if not chapters:
        safe_print("âŒ æœªæ‰¾åˆ°ç« èŠ‚åˆ—è¡¨", style="bold red")
        return False
    
    # è¿‡æ»¤ç« èŠ‚èŒƒå›´
    if range_input:
        try:
            original_count = len(chapters)
            chapters = crawler.filter_chapters_by_range(chapters, range_input)
            if not chapters:
                safe_print("âŒ æŒ‡å®šèŒƒå›´å†…æ²¡æœ‰æ‰¾åˆ°ç« èŠ‚", style="bold red")
                return False
            safe_print(f"ğŸ“– å·²ä» {original_count} ç« ä¸­ç­›é€‰å‡º {len(chapters)} ç« ", style="green")
        except Exception as e:
            safe_print(f"âŒ ç« èŠ‚èŒƒå›´è§£æå¤±è´¥: {str(e)}", style="bold red")
            return False
    
    # å¼€å§‹çˆ¬å–
    output_dir = args.output
    auto_merge = args.merge if hasattr(args, 'merge') else False
    crawler.crawl_novel(catalog_url, max_workers, chapters, output_dir, auto_merge)
    
    return True

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = create_cli_parser()
    args = parser.parse_args()
    
    # å¤„ç†ç‰¹æ®Šå‘½ä»¤
    if args.list_sites:
        show_supported_sites()
        return
    
    # æ˜¾ç¤ºç¾åŒ–çš„æ ‡é¢˜ï¼ˆåªåœ¨å¼€å§‹æ—¶æ˜¾ç¤ºä¸€æ¬¡ï¼‰
    print_banner(require_confirm=not args.yes)
    
    # ä¸»å¾ªç¯
    while True:
        try:
            # æ‰§è¡Œå•æ¬¡çˆ¬å–ä»»åŠ¡
            success = run_single_crawl(args)
            
            # å¦‚æœæ˜¯å‘½ä»¤è¡Œæ¨¡å¼ï¼ˆæŒ‡å®šäº†URLï¼‰ï¼Œæ‰§è¡Œä¸€æ¬¡åé€€å‡º
            if args.url:
                break
                
            # äº¤äº’æ¨¡å¼ï¼šè¯¢é—®æ˜¯å¦ç»§ç»­
            if not ask_continue():
                break
                
        except KeyboardInterrupt:
            safe_print("\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆï¼Œç¨‹åºé€€å‡º", style="yellow")
            break
        except Exception as e:
            safe_print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}", style="bold red")
            if not ask_continue():
                break
    
    safe_print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨é€šç”¨å°è¯´çˆ¬è™«ï¼", style="bold cyan")

if __name__ == "__main__":
    main() 