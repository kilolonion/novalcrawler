import json
import os
import random
import re
import time
import urllib.parse
import chardet
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import List, Optional, Tuple
from urllib.parse import urlparse, urljoin
from urllib.robotparser import RobotFileParser

from bs4 import BeautifulSoup

from .modules.login_manager import LoginManager
from .models import ChapterInfo, SiteConfig
from .modules.site_detector import SiteDetector
from .modules.security_checker import SecurityChecker, get_security_checker
from .utils import (RICH_AVAILABLE, console, file_lock, print_chapter_summary,
                    print_status_table, safe_print)

# å¼•å…¥å·¥å…·æ¨¡å—
from .modules import (
    detect_encoding as utils_detect_encoding,
    sanitize_filename as utils_sanitize_filename,
    is_blocked_response as utils_is_blocked_response,
    merge_chapters_to_txt as merger_merge_chapters,
    download_chapters_with_progress as downloader_progress,
    download_chapters_simple as downloader_simple,
    show_completion_stats as downloader_stats,
    fetch_and_parse_catalog as catalog_fetch,
    fetch_full_chapter_content as content_fetch_full,
    process_and_save_chapter as chapter_process_and_save,
    get_novel_title as title_get,
    get_downloaded_chapters as utils_get_downloaded,
    parse_chapter_range as utils_parse_range,
    filter_valid_chapters as catalog_filter_chapters,
)
from .modules.catalog import find_next_catalog_page as catalog_next_page

if RICH_AVAILABLE:
    from rich.progress import (BarColumn, Progress, TextColumn,
                               TimeElapsedColumn, TimeRemainingColumn)


class UniversalNovelCrawler:
    """é€šç”¨å°è¯´çˆ¬è™«"""
    
    def __init__(self, login_manager: LoginManager, detector: SiteDetector):
        self.login_manager = login_manager
        self.detector = detector
        self.session = self.login_manager.session
        self.novel_title = None
        self.security_checker = get_security_checker()
    
    def check_robots_txt(self, url: str) -> bool:
        """æ£€æŸ¥robots.txtæ˜¯å¦å…è®¸è®¿é—®"""
        # é¦–å…ˆè¿›è¡Œå®‰å…¨æ£€æŸ¥ï¼Œé˜»æ­¢è®¿é—®æ•æ„Ÿç½‘ç«™
        if not self.security_checker.validate_crawl_request(url):
            return False
            
        # å¦‚æœé…ç½®è¦æ±‚è·³è¿‡ robots.txt æ£€æŸ¥ï¼Œç›´æ¥å…è®¸
        if getattr(self, 'skip_robots', False):
            safe_print("âš ï¸ å·²æ ¹æ®å‚æ•°è·³è¿‡ robots.txt æ£€æŸ¥")
            return True
        
        try:
            parsed_url = urlparse(url)
            base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
            robots_url = urljoin(base_url, '/robots.txt')
            
            safe_print(f"ğŸ¤– æ£€æŸ¥ robots.txt: {robots_url}")
            
            # é¦–å…ˆç›´æ¥è·å–robots.txtå†…å®¹æ£€æŸ¥æ˜¯å¦è¢«Cloudflareæ‹¦æˆª
            try:
                response = self.session.get(robots_url, timeout=10)
                if self._is_blocked_response(response):
                    safe_print("âš ï¸ robots.txt è¢«åçˆ¬è™«ä¿æŠ¤æ‹¦æˆªï¼Œæ— æ³•è¯»å–")
                    safe_print("   æŒ‰ç…§HTTPåè®®ï¼Œé»˜è®¤å…è®¸è®¿é—®")
                    return True
            except Exception:
                pass
            
            # åˆ›å»ºæœºå™¨äººè§£æå™¨
            rp = RobotFileParser()
            rp.set_url(robots_url)
            
            try:
                rp.read()
                # æ£€æŸ¥æˆ‘ä»¬çš„User-Agentæ˜¯å¦è¢«å…è®¸è®¿é—®
                user_agent = self.session.headers.get('User-Agent', '*')
                can_fetch = rp.can_fetch(user_agent, url)
                
                if can_fetch:
                    safe_print("âœ… robots.txt å…è®¸è®¿é—®")
                else:
                    safe_print("âš ï¸ robots.txt ä¸å…è®¸è®¿é—®æ­¤URL")
                    safe_print("   å»ºè®®è”ç³»ç½‘ç«™ç®¡ç†å‘˜æˆ–é€‰æ‹©å…¶ä»–ç½‘ç«™")
                    
                    # ç»™ç”¨æˆ·é€‰æ‹©æ˜¯å¦ç»§ç»­çš„æœºä¼š
                    if RICH_AVAILABLE:
                        from rich.prompt import Confirm
                        continue_anyway = Confirm.ask(
                            "ğŸ¤” [yellow]æ˜¯å¦å¿½ç•¥robots.txté™åˆ¶ç»§ç»­è®¿é—®ï¼Ÿ[/yellow]", 
                            default=False
                        )
                    else:
                        continue_anyway = input("ğŸ¤” æ˜¯å¦å¿½ç•¥robots.txté™åˆ¶ç»§ç»­è®¿é—®ï¼Ÿ(y/n, é»˜è®¤n): ").strip().lower() in ['y', 'yes']
                    
                    if continue_anyway:
                        safe_print("âš ï¸ ç”¨æˆ·é€‰æ‹©å¿½ç•¥robots.txté™åˆ¶")
                        return True
                
                return can_fetch
                
            except Exception as e:
                safe_print(f"â“ æ— æ³•è¯»å– robots.txt (å¯èƒ½ä¸å­˜åœ¨): {e}")
                safe_print("   æ ¹æ®HTTPåè®®ï¼Œé»˜è®¤å…è®¸è®¿é—®")
                return True
                
        except Exception as e:
            safe_print(f"âŒ æ£€æŸ¥ robots.txt æ—¶å‡ºé”™: {e}")
            return True  # å‡ºé”™æ—¶é»˜è®¤å…è®¸
    
    def _detect_encoding(self, response) -> str:
        """ä»£ç†åˆ° modules.utils.detect_encoding"""
        return utils_detect_encoding(response)
    
    def _sanitize_filename(self, text: str) -> str:
        """ä»£ç†åˆ° modules.utils.sanitize_filename"""
        return utils_sanitize_filename(text)
    
    def _is_blocked_response(self, response) -> bool:
        """ä»£ç†åˆ° modules.utils.is_blocked_response"""
        return utils_is_blocked_response(response)
    
    def save_chapter_list(self, chapters: List[ChapterInfo], catalog_url: str):
        """ä¿å­˜ç« èŠ‚åˆ—è¡¨åˆ°JSONæ–‡ä»¶"""
        parsed_url = urlparse(catalog_url)
        site_name = parsed_url.netloc.replace('www.', '').replace('.', '_')
        filename = f"chapters_{site_name}.json"
        
        chapter_data = []
        for chapter in chapters:
            chapter_data.append({
                "title": chapter.title,
                "url": chapter.url
            })
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                "catalog_url": catalog_url,
                "total_chapters": len(chapters),
                "created_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "chapters": chapter_data
            }, f, ensure_ascii=False, indent=2)
        
        safe_print(f"ğŸ’¾ ç« èŠ‚åˆ—è¡¨å·²ä¿å­˜åˆ°: {filename}")
        return filename
    
    def load_chapter_list(self, catalog_url: str) -> Optional[List[ChapterInfo]]:
        """ä»JSONæ–‡ä»¶åŠ è½½ç« èŠ‚åˆ—è¡¨"""
        parsed_url = urlparse(catalog_url)
        site_name = parsed_url.netloc.replace('www.', '').replace('.', '_')
        filename = f"chapters_{site_name}.json"
        
        if not os.path.exists(filename):
            return None
        
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            safe_print(f"ğŸ“ æ‰¾åˆ°ç¼“å­˜æ–‡ä»¶: {filename}")
            safe_print(f"ğŸ“š ç¼“å­˜åŒ…å« {data['total_chapters']} ä¸ªç« èŠ‚")
            safe_print(f"ğŸ• åˆ›å»ºæ—¶é—´: {data['created_time']}")
            
            chapters = []
            for chapter_data in data['chapters']:
                chapters.append(ChapterInfo(
                    title=chapter_data['title'],
                    url=chapter_data['url']
                ))
            
            return chapters
            
        except Exception as e:
            safe_print(f"âŒ è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    
    def get_cache_filename(self, catalog_url: str) -> str:
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶å"""
        parsed_url = urlparse(catalog_url)
        site_name = parsed_url.netloc.replace('www.', '').replace('.', '_')
        return f"chapters_{site_name}.json"
    
    def detect_and_fix_chapter_order(self, chapters: List[ChapterInfo]) -> List[ChapterInfo]:
        """æ£€æµ‹å¹¶ä¿®æ­£ç« èŠ‚é¡ºåº"""
        if len(chapters) < 2:
            return chapters
        
        # æå–å‰å‡ ä¸ªç« èŠ‚çš„æ•°å­—
        first_nums = []
        for i in range(min(5, len(chapters))):
            title = chapters[i].title
            numbers = re.findall(r'ç¬¬(\d+)ç« ', title)
            if numbers:
                first_nums.append((i, int(numbers[0])))
        
        if len(first_nums) >= 2:
            # æ£€æŸ¥æ˜¯å¦ä¸ºå€’åº
            first_chapter_num = first_nums[0][1]
            second_chapter_num = first_nums[1][1]
            
            if first_chapter_num > second_chapter_num:
                safe_print("ğŸ”„ æ£€æµ‹åˆ°ç« èŠ‚åˆ—è¡¨ä¸ºå€’åºï¼Œæ­£åœ¨ä¿®æ­£...")
                chapters.reverse()
                
                # é‡æ–°æå–ä¿®æ­£åçš„ç« èŠ‚å·
                first_title = chapters[0].title
                last_title = chapters[-1].title
                first_num = re.findall(r'ç¬¬(\d+)ç« ', first_title)
                last_num = re.findall(r'ç¬¬(\d+)ç« ', last_title)
                
                first_str = first_num[0] if first_num else '?'
                last_str = last_num[0] if last_num else '?'
                
                safe_print(f"âœ… ç« èŠ‚é¡ºåºå·²ä¿®æ­£ï¼šç¬¬{first_str}ç«  -> ç¬¬{last_str}ç« ")
            else:
                # æ˜¾ç¤ºå½“å‰ç« èŠ‚èŒƒå›´
                first_title = chapters[0].title
                last_title = chapters[-1].title
                first_num = re.findall(r'ç¬¬(\d+)ç« ', first_title)
                last_num = re.findall(r'ç¬¬(\d+)ç« ', last_title)
                
                if first_num and last_num:
                    safe_print(f"ğŸ“Š ç« èŠ‚èŒƒå›´ï¼šç¬¬{first_num[0]}ç«  - ç¬¬{last_num[0]}ç« ")
        
        return chapters
    
    def get_chapter_list_from_url(self, catalog_url: str) -> List[ChapterInfo]:
        """ä»URLè·å–ç« èŠ‚åˆ—è¡¨å¹¶ç¼“å­˜"""
        # é’ˆå¯¹ç‰¹å®šç½‘ç«™çš„URLè½¬æ¢
        original_url = catalog_url
        if 'huanqixiaoshuo.com' in catalog_url and not catalog_url.endswith('/all.html'):
            # è½¬æ¢ä¸ºç›®å½•é¡µé¢URL
            if catalog_url.endswith('/'):
                catalog_url = catalog_url + 'all.html'
            else:
                catalog_url = catalog_url + '/all.html'
            safe_print(f"ğŸ”„ è‡ªåŠ¨è½¬æ¢ä¸ºç›®å½•é¡µ: {catalog_url}")
        
        safe_print(f"ğŸ” æ­£åœ¨åˆ†æç›®å½•é¡µé¢: {catalog_url}")
        
        # æ£€æŸ¥ç¼“å­˜ - ä½¿ç”¨è½¬æ¢åçš„URL
        cache_file = self.get_cache_filename(catalog_url)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                chapters = [ChapterInfo(title=item['title'], url=item['url']) for item in data['chapters']]
                safe_print(f"ğŸ“ ä»ç¼“å­˜åŠ è½½ {len(chapters)} ä¸ªç« èŠ‚")
                return chapters
            except Exception as e:
                safe_print(f"âš ï¸  ç¼“å­˜æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        
        # è·å–æ–°çš„ç« èŠ‚åˆ—è¡¨
        chapters = self.get_chapter_list(catalog_url)
        
        # æ£€æµ‹å¹¶ä¿®æ­£ç« èŠ‚é¡ºåº
        if chapters:
            chapters = self.detect_and_fix_chapter_order(chapters)
        
        # åº”ç”¨æ”¹è¿›çš„ç« èŠ‚è¿‡æ»¤
        if chapters:
            filtered_chapters = self.filter_valid_chapters(chapters)
            
            if filtered_chapters:
                safe_print(f"ğŸ” è¿‡æ»¤åä¿ç•™ {len(filtered_chapters)} ä¸ªæœ‰æ•ˆç« èŠ‚")
                chapters = filtered_chapters
        
        # ä¿å­˜åˆ°ç¼“å­˜
        if chapters:
            try:
                cache_data = {
                    'url': catalog_url,
                    'timestamp': time.time(),
                    'chapters': [{'title': chapter.title, 'url': chapter.url} for chapter in chapters]
                }
                with open(cache_file, 'w', encoding='utf-8') as f:
                    json.dump(cache_data, f, ensure_ascii=False, indent=2)
                safe_print(f"ğŸ’¾ ç« èŠ‚åˆ—è¡¨å·²ä¿å­˜åˆ°: {cache_file}")
            except Exception as e:
                safe_print(f"âš ï¸  ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        
        return chapters
    
    def get_chapter_list(self, catalog_url: str) -> List[ChapterInfo]:
        """
        è·å–å°è¯´ç« èŠ‚åˆ—è¡¨çš„å…¥å£ç‚¹ã€‚
        è¯¥æ–¹æ³•ä¼šè°ƒç”¨æ¨¡å—åŒ–çš„ç›®å½•è·å–ä¸è§£æå‡½æ•°ã€‚
        """
        # é¦–å…ˆè¿›è¡Œå®‰å…¨æ£€æŸ¥å’Œrobots.txtéªŒè¯
        if not self.check_robots_txt(catalog_url):
            safe_print("âŒ å®‰å…¨æ£€æŸ¥æˆ–robots.txtéªŒè¯å¤±è´¥ï¼Œæ‹’ç»è®¿é—®")
            return []
            
        return catalog_fetch(
            catalog_url=catalog_url,
            session=self.session,
            detector=self.detector,
            headers=self.headers
        )

    def filter_chapters_by_range(self, chapters: List[ChapterInfo], range_input: str) -> List[ChapterInfo]:
        """
        æ ¹æ®ç”¨æˆ·è¾“å…¥çš„èŒƒå›´è¿‡æ»¤ç« èŠ‚åˆ—è¡¨
        """
        start, end = utils_parse_range(range_input, len(chapters))
        return chapters[start-1:end]  

    def crawl_single_chapter(self, chapter: ChapterInfo, output_dir: str, silent: bool = False) -> Optional[str]:
        """
        æŠ“å–å¹¶ä¿å­˜å•ä¸ªç« èŠ‚çš„å…¥å£ç‚¹ã€‚
        è¯¥æ–¹æ³•è°ƒç”¨æ¨¡å—åŒ–çš„ç« èŠ‚å¤„ç†å‡½æ•°ã€‚
        """
        return chapter_process_and_save(
            chapter=chapter,
            output_dir=output_dir,
            detector=self.detector,
            session=self.session,
            headers=self.headers,
            silent=silent
        )

    def _fetch_chapter_content(self, chapter_url: str) -> str:
        """è·å–å¹¶æ‹¼æ¥å•ç« èŠ‚æ‰€æœ‰åˆ†é¡µçš„HTMLå†…å®¹ã€‚"""
        return content_fetch_full(
            chapter_url=chapter_url,
            session=self.session,
            detector=self.detector,
            headers=self.headers,
        )

    def crawl_novel(self, catalog_url: str, max_workers: int = 3, chapters: List[ChapterInfo] = None, output_dir: str = None, auto_merge: bool = False, chapter_range: str = None):
        """çˆ¬å–å°è¯´å¹¶ä¿å­˜"""
        if not chapters:
            chapters = self.get_chapter_list(catalog_url)
        
        if not chapters:
            return

        # è¿‡æ»¤æ— æ•ˆç« èŠ‚
        chapters = catalog_filter_chapters(chapters)

        # å¤„ç†ç”¨æˆ·æŒ‡å®šçš„ç« èŠ‚èŒƒå›´
        if chapter_range:
            try:
                start, end = utils_parse_range(chapter_range, len(chapters))
                chapters = chapters[start-1:end]
                safe_print(f"ğŸ“– å·²é€‰æ‹©ç« èŠ‚èŒƒå›´: {start} åˆ° {end} (å…± {len(chapters)} ç« )")
            except ValueError as e:
                safe_print(f"âŒ [red]ç« èŠ‚èŒƒå›´é”™è¯¯: {e}[/red]")
                return

        original_chapter_count = len(chapters)

        # è·å–å°è¯´æ ‡é¢˜ï¼Œç”¨äºç”Ÿæˆé»˜è®¤ç›®å½•åå’Œåˆå¹¶æ–‡ä»¶å
        self.novel_title = title_get(
            url=catalog_url,
            session=self.session,
            headers=self.headers,
        )
        if not self.novel_title:
            # å¦‚æœæ— æ³•è·å–æ ‡é¢˜ï¼Œåˆ™ä½¿ç”¨URLä¸­çš„ä¸€éƒ¨åˆ†ä½œä¸ºåå¤‡
            self.novel_title = urlparse(catalog_url).netloc.replace('.', '_')

        if not output_dir:
            output_dir = self._sanitize_filename(self.novel_title)
        os.makedirs(output_dir, exist_ok=True)
        safe_print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

        # --- æ–­ç‚¹ç»­ä¼ æ ¸å¿ƒé€»è¾‘ ---
        # è·å–å·²ä¸‹è½½ç« èŠ‚åˆ—è¡¨
        downloaded_chapters = utils_get_downloaded(output_dir)
        
        if downloaded_chapters:
            safe_print(f"ğŸ” æ£€æµ‹åˆ° {len(downloaded_chapters)} ä¸ªå·²ä¸‹è½½ç« èŠ‚ï¼Œå°†è¿›è¡Œæ–­ç‚¹ç»­ä¼ ã€‚")
            chapters_to_download = [
                ch for ch in chapters if self._sanitize_filename(ch.title) not in downloaded_chapters
            ]
            skipped_count = len(chapters) - len(chapters_to_download)
        else:
            chapters_to_download = chapters
            skipped_count = 0

        if not chapters_to_download:
            downloader_stats(0, 0, skipped_count, output_dir, lambda o: utils_get_downloaded(o))
            if auto_merge or self._ask_merge_chapters():
                merger_merge_chapters(
                    output_dir,
                    getattr(self, 'novel_title', None),
                    self._sanitize_filename
                )
            return
        # --- æ–­ç‚¹ç»­ä¼ æ ¸å¿ƒé€»è¾‘ç»“æŸ ---

        if RICH_AVAILABLE:
            success_count = downloader_progress(
                chapters_to_download, 
                output_dir, 
                max_workers,
                total_chapters=original_chapter_count,
                initial_advance=skipped_count,
                crawl_func=self.crawl_single_chapter
            )
        else:
            success_count = downloader_simple(
                chapters_to_download,
                output_dir,
                max_workers,
                crawl_func=self.crawl_single_chapter
            )

        downloader_stats(
            success_count, 
            len(chapters_to_download), 
            skipped_count, 
            output_dir, 
            lambda o: utils_get_downloaded(o)
        )

        if auto_merge or self._ask_merge_chapters():
            merger_merge_chapters(
                output_dir,
                getattr(self, 'novel_title', None),
                self._sanitize_filename
            )

    def _should_continue_download(self) -> bool:
        """è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­ä¸‹è½½"""
        if RICH_AVAILABLE:
            from rich.prompt import Confirm
            return Confirm.ask("ğŸ¤” æ˜¯å¦ç»§ç»­ä¸‹è½½å‰©ä½™ç« èŠ‚ï¼Ÿ[bold green](y/n, é»˜è®¤y)[/bold green]: ", default=True)
        else:
            return input("ğŸ¤” æ˜¯å¦ç»§ç»­ä¸‹è½½å‰©ä½™ç« èŠ‚ï¼Ÿ(y/n, é»˜è®¤y): ").strip().lower() in ['', 'y', 'yes']
    
    def _ask_merge_chapters(self) -> bool:
        """è¯¢é—®ç”¨æˆ·æ˜¯å¦åˆå¹¶ç« èŠ‚"""
        if RICH_AVAILABLE and console:
            from rich.panel import Panel
            from rich import box
            from rich.prompt import Confirm
            
            merge_help_text = (
                "å°†æ‰€æœ‰ä¸‹è½½çš„ç« èŠ‚åˆå¹¶æˆä¸€ä¸ª .txt æ–‡ä»¶ã€‚\n"
                "â€¢ ç« èŠ‚æ ‡é¢˜å°†è‡ªåŠ¨æ ¼å¼åŒ–\n"
                "â€¢ æ­£æ–‡å†…å®¹å°†è‡ªåŠ¨ç¼©è¿›\n"
                "â€¢ åˆ†é¡µæ ‡è®°å’Œæ— å…³å†…å®¹å°†è¢«ç§»é™¤"
            )
            
            panel = Panel(
                merge_help_text,
                title="[bold cyan]ğŸ“š ç« èŠ‚åˆå¹¶åŠŸèƒ½[/bold cyan]",
                border_style="cyan",
                expand=False
            )
            console.print(panel)
            
            return Confirm.ask("ğŸ“– [bold green]æ˜¯å¦æ‰§è¡Œåˆå¹¶ï¼Ÿ[/bold green]", default=True)
        else:
            print("\nğŸ“š ç« èŠ‚åˆå¹¶åŠŸèƒ½:")
            print("  â€¢ å°†æ‰€æœ‰.mdç« èŠ‚æ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ª.txtæ–‡ä»¶")
            return input("ğŸ¤” æ˜¯å¦åˆå¹¶æ‰€æœ‰ç« èŠ‚ä¸ºä¸€ä¸ªtxtæ–‡ä»¶ï¼Ÿ(y/n, é»˜è®¤n): ").strip().lower() in ['y', 'yes']
    
    @property
    def headers(self) -> dict:
        """è·å–è¯·æ±‚å¤´"""
        return self.session.headers

   