import json
import os
import random
import re
import time
import urllib.parse
import chardet
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import List, Optional, Tuple
from urllib.parse import urlparse, urljoin
from urllib.robotparser import RobotFileParser

from bs4 import BeautifulSoup

from .login_manager import LoginManager
from .models import ChapterInfo, SiteConfig
from .site_detector import SiteDetector
from .utils import (RICH_AVAILABLE, console, file_lock, print_chapter_summary,
                    print_status_table, safe_print)

if RICH_AVAILABLE:
    from rich.progress import (BarColumn, Progress, TextColumn,
                               TimeElapsedColumn, TimeRemainingColumn)


class UniversalNovelCrawler:
    """é€šç”¨å°è¯´çˆ¬è™«"""
    
    def __init__(self, login_manager: LoginManager, detector: SiteDetector):
        self.login_manager = login_manager
        self.detector = detector
        self.session = self.login_manager.session
    
    def check_robots_txt(self, url: str) -> bool:
        """æ£€æŸ¥robots.txtæ˜¯å¦å…è®¸è®¿é—®"""
        # å¦‚æœé…ç½®è¦æ±‚è·³è¿‡ robots.txt æ£€æŸ¥ï¼Œç›´æ¥å…è®¸
        if getattr(self, 'skip_robots', False):
            safe_print("âš ï¸ å·²æ ¹æ®å‚æ•°è·³è¿‡ robots.txt æ£€æŸ¥")
            return True
        
        try:
            parsed_url = urlparse(url)
            base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
            robots_url = urljoin(base_url, '/robots.txt')
            
            safe_print(f"ğŸ¤– æ£€æŸ¥ robots.txt: {robots_url}")
            
            # é¦–å…ˆç›´æ¥è·å–robots.txtå†…å®¹æ£€æŸ¥æ˜¯å¦è¢«Cloudflareæ‹¦æˆª
            try:
                response = self.session.get(robots_url, timeout=10)
                if self._is_blocked_response(response):
                    safe_print("âš ï¸ robots.txt è¢«åçˆ¬è™«ä¿æŠ¤æ‹¦æˆªï¼Œæ— æ³•è¯»å–")
                    safe_print("   æŒ‰ç…§HTTPåè®®ï¼Œé»˜è®¤å…è®¸è®¿é—®")
                    return True
            except Exception:
                pass
            
            # åˆ›å»ºæœºå™¨äººè§£æå™¨
            rp = RobotFileParser()
            rp.set_url(robots_url)
            
            try:
                rp.read()
                # æ£€æŸ¥æˆ‘ä»¬çš„User-Agentæ˜¯å¦è¢«å…è®¸è®¿é—®
                user_agent = self.session.headers.get('User-Agent', '*')
                can_fetch = rp.can_fetch(user_agent, url)
                
                if can_fetch:
                    safe_print("âœ… robots.txt å…è®¸è®¿é—®")
                else:
                    safe_print("âš ï¸ robots.txt ä¸å…è®¸è®¿é—®æ­¤URL")
                    safe_print("   å»ºè®®è”ç³»ç½‘ç«™ç®¡ç†å‘˜æˆ–é€‰æ‹©å…¶ä»–ç½‘ç«™")
                    
                    # ç»™ç”¨æˆ·é€‰æ‹©æ˜¯å¦ç»§ç»­çš„æœºä¼š
                    if RICH_AVAILABLE:
                        from rich.prompt import Confirm
                        continue_anyway = Confirm.ask(
                            "ğŸ¤” [yellow]æ˜¯å¦å¿½ç•¥robots.txté™åˆ¶ç»§ç»­è®¿é—®ï¼Ÿ[/yellow]", 
                            default=False
                        )
                    else:
                        continue_anyway = input("ğŸ¤” æ˜¯å¦å¿½ç•¥robots.txté™åˆ¶ç»§ç»­è®¿é—®ï¼Ÿ(y/n, é»˜è®¤n): ").strip().lower() in ['y', 'yes']
                    
                    if continue_anyway:
                        safe_print("âš ï¸ ç”¨æˆ·é€‰æ‹©å¿½ç•¥robots.txté™åˆ¶")
                        return True
                
                return can_fetch
                
            except Exception as e:
                safe_print(f"â“ æ— æ³•è¯»å– robots.txt (å¯èƒ½ä¸å­˜åœ¨): {e}")
                safe_print("   æ ¹æ®HTTPåè®®ï¼Œé»˜è®¤å…è®¸è®¿é—®")
                return True
                
        except Exception as e:
            safe_print(f"âŒ æ£€æŸ¥ robots.txt æ—¶å‡ºé”™: {e}")
            return True  # å‡ºé”™æ—¶é»˜è®¤å…è®¸
    
    def _detect_encoding(self, response) -> str:
        """æ™ºèƒ½æ£€æµ‹ç½‘é¡µç¼–ç """
        # 1. å°è¯•ä»HTTPå¤´è·å–ç¼–ç 
        content_type = response.headers.get('content-type', '').lower()
        if 'charset=' in content_type:
            charset = content_type.split('charset=')[1].split(';')[0].strip()
            if charset:
                return charset
        
        # 2. å°è¯•ä»HTML metaæ ‡ç­¾è·å–ç¼–ç 
        content_preview = response.content[:2048]  # åªæ£€æŸ¥å‰2KB
        try:
            soup = BeautifulSoup(content_preview, 'html.parser')
            
            # æŸ¥æ‰¾ <meta charset="...">
            meta_charset = soup.find('meta', attrs={'charset': True})
            if meta_charset:
                return meta_charset.get('charset')
            
            # æŸ¥æ‰¾ <meta http-equiv="content-type" content="...">
            meta_content_type = soup.find('meta', attrs={'http-equiv': lambda x: x and x.lower() == 'content-type'})
            if meta_content_type:
                content = meta_content_type.get('content', '').lower()
                if 'charset=' in content:
                    charset = content.split('charset=')[1].split(';')[0].strip()
                    if charset:
                        return charset
        except:
            pass
        
        # 3. ä½¿ç”¨chardetè‡ªåŠ¨æ£€æµ‹
        try:
            detected = chardet.detect(response.content[:10240])  # æ£€æŸ¥å‰10KB
            if detected and detected['encoding'] and detected['confidence'] > 0.7:
                return detected['encoding']
        except:
            pass
        
        # 4. é»˜è®¤å›é€€åˆ°UTF-8
        return 'utf-8'
    
    def _sanitize_filename(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ä½œä¸ºå®‰å…¨çš„æ–‡ä»¶å"""
        return re.sub(r'[\\/*?:"<>|]', "", text).strip()
    
    def save_chapter_list(self, chapters: List[ChapterInfo], catalog_url: str):
        """ä¿å­˜ç« èŠ‚åˆ—è¡¨åˆ°JSONæ–‡ä»¶"""
        parsed_url = urlparse(catalog_url)
        site_name = parsed_url.netloc.replace('www.', '').replace('.', '_')
        filename = f"chapters_{site_name}.json"
        
        chapter_data = []
        for chapter in chapters:
            chapter_data.append({
                "title": chapter.title,
                "url": chapter.url
            })
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                "catalog_url": catalog_url,
                "total_chapters": len(chapters),
                "created_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "chapters": chapter_data
            }, f, ensure_ascii=False, indent=2)
        
        safe_print(f"ğŸ’¾ ç« èŠ‚åˆ—è¡¨å·²ä¿å­˜åˆ°: {filename}")
        return filename
    
    def load_chapter_list(self, catalog_url: str) -> Optional[List[ChapterInfo]]:
        """ä»JSONæ–‡ä»¶åŠ è½½ç« èŠ‚åˆ—è¡¨"""
        parsed_url = urlparse(catalog_url)
        site_name = parsed_url.netloc.replace('www.', '').replace('.', '_')
        filename = f"chapters_{site_name}.json"
        
        if not os.path.exists(filename):
            return None
        
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            safe_print(f"ğŸ“ æ‰¾åˆ°ç¼“å­˜æ–‡ä»¶: {filename}")
            safe_print(f"ğŸ“š ç¼“å­˜åŒ…å« {data['total_chapters']} ä¸ªç« èŠ‚")
            safe_print(f"ğŸ• åˆ›å»ºæ—¶é—´: {data['created_time']}")
            
            chapters = []
            for chapter_data in data['chapters']:
                chapters.append(ChapterInfo(
                    title=chapter_data['title'],
                    url=chapter_data['url']
                ))
            
            return chapters
            
        except Exception as e:
            safe_print(f"âŒ è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    
    def get_downloaded_chapters(self, output_dir: str) -> set:
        """è·å–å·²ä¸‹è½½çš„ç« èŠ‚æ ‡é¢˜"""
        if not os.path.exists(output_dir):
            return set()
        
        downloaded = set()
        for filename in os.listdir(output_dir):
            if filename.endswith('.md'):
                # ç§»é™¤.mdåç¼€ï¼Œå¾—åˆ°ç« èŠ‚æ ‡é¢˜
                title = filename[:-3]
                downloaded.add(title)
        
        return downloaded
    
    def get_cache_filename(self, catalog_url: str) -> str:
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶å"""
        parsed_url = urlparse(catalog_url)
        site_name = parsed_url.netloc.replace('www.', '').replace('.', '_')
        return f"chapters_{site_name}.json"
    
    def detect_and_fix_chapter_order(self, chapters: List[ChapterInfo]) -> List[ChapterInfo]:
        """æ£€æµ‹å¹¶ä¿®æ­£ç« èŠ‚é¡ºåº"""
        if len(chapters) < 2:
            return chapters
        
        # æå–å‰å‡ ä¸ªç« èŠ‚çš„æ•°å­—
        first_nums = []
        for i in range(min(5, len(chapters))):
            title = chapters[i].title
            numbers = re.findall(r'ç¬¬(\d+)ç« ', title)
            if numbers:
                first_nums.append((i, int(numbers[0])))
        
        if len(first_nums) >= 2:
            # æ£€æŸ¥æ˜¯å¦ä¸ºå€’åº
            first_chapter_num = first_nums[0][1]
            second_chapter_num = first_nums[1][1]
            
            if first_chapter_num > second_chapter_num:
                safe_print("ğŸ”„ æ£€æµ‹åˆ°ç« èŠ‚åˆ—è¡¨ä¸ºå€’åºï¼Œæ­£åœ¨ä¿®æ­£...")
                chapters.reverse()
                
                # é‡æ–°æå–ä¿®æ­£åçš„ç« èŠ‚å·
                first_title = chapters[0].title
                last_title = chapters[-1].title
                first_num = re.findall(r'ç¬¬(\d+)ç« ', first_title)
                last_num = re.findall(r'ç¬¬(\d+)ç« ', last_title)
                
                first_str = first_num[0] if first_num else '?'
                last_str = last_num[0] if last_num else '?'
                
                safe_print(f"âœ… ç« èŠ‚é¡ºåºå·²ä¿®æ­£ï¼šç¬¬{first_str}ç«  -> ç¬¬{last_str}ç« ")
            else:
                # æ˜¾ç¤ºå½“å‰ç« èŠ‚èŒƒå›´
                first_title = chapters[0].title
                last_title = chapters[-1].title
                first_num = re.findall(r'ç¬¬(\d+)ç« ', first_title)
                last_num = re.findall(r'ç¬¬(\d+)ç« ', last_title)
                
                if first_num and last_num:
                    safe_print(f"ğŸ“Š ç« èŠ‚èŒƒå›´ï¼šç¬¬{first_num[0]}ç«  - ç¬¬{last_num[0]}ç« ")
        
        return chapters
    
    def get_chapter_list_from_url(self, catalog_url: str) -> List[ChapterInfo]:
        """ä»URLè·å–ç« èŠ‚åˆ—è¡¨å¹¶ç¼“å­˜"""
        # é’ˆå¯¹ç‰¹å®šç½‘ç«™çš„URLè½¬æ¢
        original_url = catalog_url
        if 'huanqixiaoshuo.com' in catalog_url and not catalog_url.endswith('/all.html'):
            # è½¬æ¢ä¸ºç›®å½•é¡µé¢URL
            if catalog_url.endswith('/'):
                catalog_url = catalog_url + 'all.html'
            else:
                catalog_url = catalog_url + '/all.html'
            safe_print(f"ğŸ”„ è‡ªåŠ¨è½¬æ¢ä¸ºç›®å½•é¡µ: {catalog_url}")
        
        safe_print(f"ğŸ” æ­£åœ¨åˆ†æç›®å½•é¡µé¢: {catalog_url}")
        
        # æ£€æŸ¥ç¼“å­˜ - ä½¿ç”¨è½¬æ¢åçš„URL
        cache_file = self.get_cache_filename(catalog_url)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                chapters = [ChapterInfo(title=item['title'], url=item['url']) for item in data['chapters']]
                safe_print(f"ğŸ“ ä»ç¼“å­˜åŠ è½½ {len(chapters)} ä¸ªç« èŠ‚")
                return chapters
            except Exception as e:
                safe_print(f"âš ï¸  ç¼“å­˜æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        
        # è·å–æ–°çš„ç« èŠ‚åˆ—è¡¨
        chapters = self.get_chapter_list(catalog_url)
        
        # æ£€æµ‹å¹¶ä¿®æ­£ç« èŠ‚é¡ºåº
        if chapters:
            chapters = self.detect_and_fix_chapter_order(chapters)
        
        # åº”ç”¨æ”¹è¿›çš„ç« èŠ‚è¿‡æ»¤
        if chapters:
            filtered_chapters = self.filter_valid_chapters(chapters)
            
            if filtered_chapters:
                safe_print(f"ğŸ” è¿‡æ»¤åä¿ç•™ {len(filtered_chapters)} ä¸ªæœ‰æ•ˆç« èŠ‚")
                chapters = filtered_chapters
        
        # ä¿å­˜åˆ°ç¼“å­˜
        if chapters:
            try:
                cache_data = {
                    'url': catalog_url,
                    'timestamp': time.time(),
                    'chapters': [{'title': chapter.title, 'url': chapter.url} for chapter in chapters]
                }
                with open(cache_file, 'w', encoding='utf-8') as f:
                    json.dump(cache_data, f, ensure_ascii=False, indent=2)
                safe_print(f"ğŸ’¾ ç« èŠ‚åˆ—è¡¨å·²ä¿å­˜åˆ°: {cache_file}")
            except Exception as e:
                safe_print(f"âš ï¸  ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        
        return chapters
    
    def get_chapter_list(self, catalog_url: str) -> List[ChapterInfo]:
        """è·å–ç« èŠ‚åˆ—è¡¨çš„æ ¸å¿ƒé€»è¾‘"""
        # é¦–å…ˆæ£€æŸ¥robots.txt
        if not self.check_robots_txt(catalog_url):
            safe_print("âŒ robots.txt ä¸å…è®¸è®¿é—®ï¼Œç¨‹åºé€€å‡º")
            return []
        
        config = self.detector.detect_site(catalog_url)
        if not config:
            safe_print("âŒ æ— æ³•è¯†åˆ«ç½‘ç«™ç±»å‹")
            return []
        
        try:
            response = self.session.get(catalog_url, timeout=10)
            
            # æ™ºèƒ½æ£€æµ‹ç¼–ç 
            detected_encoding = self._detect_encoding(response)
            response.encoding = detected_encoding
            safe_print(f"ğŸ” æ£€æµ‹åˆ°ç½‘é¡µç¼–ç : {detected_encoding}")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–å¹¶ä¿å­˜å°è¯´æ ‡é¢˜ï¼ˆä¾›åç»­åˆå¹¶ä½¿ç”¨ï¼‰
            novel_title = self._extract_novel_title(soup, catalog_url)
            if novel_title:
                self.novel_title = novel_title
            
            chapters = []
            
            # ç‰¹æ®Šå¤„ç†ï¼šhuanqixiaoshuo.comç½‘ç«™
            if 'huanqixiaoshuo.com' in catalog_url:
                # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                all_links = soup.find_all("a", href=True)
                for link in all_links:
                    href = link.get("href")
                    text = link.get_text(strip=True)
                    
                    if not text or not href:
                        continue
                    
                    # å¤„ç†JavaScripté“¾æ¥
                    if href.startswith("javascript:"):
                        js_match = re.search(r"javascript:gobook\(['\"](\d+)['\"],\s*['\"](\d+)['\"],\s*['\"](\d+)['\"]\)", href)
                        if js_match:
                            book_id = js_match.group(1)  # é€šå¸¸æ˜¯ "2"
                            novel_id = js_match.group(2)  # å°è¯´ID
                            chapter_id = js_match.group(3)  # ç« èŠ‚ID
                            chapter_url = f"https://www.huanqixiaoshuo.com/{book_id}_{novel_id}/{chapter_id}.html"
                            chapters.append(ChapterInfo(title=text, url=chapter_url))
                    
                    # å¤„ç†æ™®é€šé“¾æ¥
                    elif "/2_" in href and ".html" in href:
                        if href.startswith("/"):
                            chapter_url = f"https://www.huanqixiaoshuo.com{href}"
                        else:
                            chapter_url = href
                        chapters.append(ChapterInfo(title=text, url=chapter_url))
                
                safe_print(f"âœ… ç‰¹æ®Šå¤„ç†huanqixiaoshuo.comæ‰¾åˆ° {len(chapters)} ä¸ªé“¾æ¥")
                
            else:
                # é€šç”¨å¤„ç†é€»è¾‘
                # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨
                for selector in config.catalog_selectors:
                    try:
                        links = soup.select(selector)
                        if links:
                            safe_print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
                            break
                    except Exception as e:
                        continue
                else:
                    safe_print("âŒ æœªæ‰¾åˆ°ç« èŠ‚é“¾æ¥")
                    return []
                
                base_url = f"{urlparse(catalog_url).scheme}://{urlparse(catalog_url).netloc}"
                
                for link in links:
                    try:
                        title = link.get_text(strip=True)
                        href = link.get('href', '')
                        
                        if not title or not href:
                            continue
                        
                        # æ„é€ å®Œæ•´URL
                        if href.startswith('http'):
                            chapter_url = href
                        elif href.startswith('/'):
                            chapter_url = base_url + href
                        else:
                            chapter_url = catalog_url.rstrip('/') + '/' + href.lstrip('/')
                        
                        chapters.append(ChapterInfo(title=title, url=chapter_url))
                        
                    except Exception as e:
                        continue
            
            # å»é‡å’Œæ’åº
            seen_urls = set()
            unique_chapters = []
            for chapter in chapters:
                if chapter.url not in seen_urls:
                    seen_urls.add(chapter.url)
                    unique_chapters.append(chapter)
            
            safe_print(f"ğŸ“š åˆæ­¥è·å– {len(unique_chapters)} ä¸ªç« èŠ‚é“¾æ¥")
            return unique_chapters
            
        except Exception as e:
            safe_print(f"âŒ è·å–ç« èŠ‚åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []
    
    def crawl_single_chapter(self, chapter: ChapterInfo, output_dir: str, silent: bool = False) -> Optional[str]:
        """ä¸‹è½½å¹¶ä¿å­˜å•ä¸ªç« èŠ‚
        
        Args:
            chapter: ç« èŠ‚ä¿¡æ¯
            output_dir: è¾“å‡ºç›®å½•
            silent: æ˜¯å¦é™é»˜æ¨¡å¼ï¼ˆä¸è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼‰
        """
        
        # æ¸…ç†æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
        safe_title = self._sanitize_filename(chapter.title)
        filename = f"{safe_title}.md"
        filepath = os.path.join(output_dir, filename)
        
        # ä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿çº¿ç¨‹å®‰å…¨
        lock_path = f"{filepath}.lock"
        with file_lock(lock_path):
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(filepath):
                return "skipped"
            
            # è·å–ç« èŠ‚å†…å®¹
            content = self._fetch_chapter_content(chapter.url, chapter.title, silent=silent)
            if not content:
                return None
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            
            # ä¿å­˜æ–‡ä»¶
            try:
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(f"# {chapter.title}\n\n{content}")
                return filepath
            except Exception as e:
                if not silent:
                    safe_print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
                return None

    def _fetch_chapter_content(self, url: str, chapter_title: str, silent: bool = False) -> Optional[str]:
        """è·å–ç« èŠ‚å†…å®¹
        
        Args:
            url: ç« èŠ‚URL
            chapter_title: ç« èŠ‚æ ‡é¢˜
            silent: æ˜¯å¦é™é»˜æ¨¡å¼ï¼ˆä¸è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼‰
        """
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # æ£€æµ‹åçˆ¬è™«ä¿æŠ¤
            if self._is_blocked_response(response):
                if not silent:
                    safe_print(f"âŒ ç½‘ç«™å¯ç”¨äº†åçˆ¬è™«ä¿æŠ¤ (Cloudflare/å…¶ä»–)")
                return None
            
            # æ™ºèƒ½æ£€æµ‹ç¼–ç 
            detected_encoding = self._detect_encoding(response)
            response.encoding = detected_encoding
            
            if not silent:
                safe_print(f"ğŸ” æ£€æµ‹åˆ°ç½‘é¡µç¼–ç : {detected_encoding}")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ£€æµ‹ç½‘ç«™é…ç½®
            config = self.detector.detect_site(url)
            if not silent:
                safe_print(f"ğŸ¯ æ£€æµ‹åˆ°ç½‘ç«™ç±»å‹: {config.name}")
            
            all_content = []
            current_url = url
            page_num = 1
            max_pages = 20  # é˜²æ­¢æ— é™å¾ªç¯
            
            while current_url and page_num <= max_pages:
                if page_num > 1:
                    try:
                        response = self.session.get(current_url, timeout=30)
                        response.raise_for_status()
                        
                        # å†æ¬¡æ£€æµ‹åçˆ¬è™«ä¿æŠ¤
                        if self._is_blocked_response(response):
                            if not silent:
                                safe_print(f"âŒ ç¬¬{page_num}é¡µè¢«åçˆ¬è™«ä¿æŠ¤æ‹¦æˆª")
                            break
                            
                        response.encoding = detected_encoding
                        soup = BeautifulSoup(response.text, 'html.parser')
                    except Exception as e:
                        if not silent:
                            safe_print(f"âŒ è·å–ç¬¬{page_num}é¡µå¤±è´¥: {e}")
                        break
                
                # æå–å½“å‰é¡µå†…å®¹
                content = self._extract_content(soup, config)
                if content:
                    all_content.extend(content)
                    if not silent:
                        safe_print(f"    ğŸ“„ ç¬¬{page_num}é¡µ: æå–äº† {len(content)} ä¸ªæ®µè½")
                else:
                    if not silent:
                        safe_print(f"    âš ï¸ ç¬¬{page_num}é¡µ: æœªæ‰¾åˆ°å†…å®¹")
                    break
                
                # æŸ¥æ‰¾ä¸‹ä¸€é¡µ
                next_url = self._find_next_page(soup, current_url, config)
                if not next_url or next_url == current_url:
                    break
                    
                current_url = next_url
                page_num += 1
                
                # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(random.uniform(0.5, 1.5))
            
            if all_content:
                # æ¸…ç†å†…å®¹
                cleaned_content = self._clean_content(all_content, config)
                return '\n\n'.join(cleaned_content)
            else:
                if not silent:
                    safe_print(f"âŒ ç« èŠ‚ '{chapter_title}' å†…å®¹ä¸ºç©º")
                return None
                
        except Exception as e:
            if not silent:
                safe_print(f"âŒ è·å–ç« èŠ‚å†…å®¹å¤±è´¥: {e}")
            return None
    
    def _is_blocked_response(self, response) -> bool:
        """æ£€æµ‹æ˜¯å¦è¢«åçˆ¬è™«ä¿æŠ¤æ‹¦æˆª"""
        # æ£€æµ‹CloudflareéªŒè¯é¡µé¢
        if response.status_code == 403:
            return True
        
        # æ£€æµ‹Cloudflareç‰¹å¾
        content_lower = response.text.lower()
        cloudflare_indicators = [
            "just a moment",
            "checking your browser",
            "cloudflare",
            "ddos protection",
            "security check",
            "human verification"
        ]
        
        if any(indicator in content_lower for indicator in cloudflare_indicators):
            return True
        
        # æ£€æµ‹å…¶ä»–åçˆ¬è™«ä¿æŠ¤
        if len(response.text) < 500 and ("blocked" in content_lower or "forbidden" in content_lower):
            return True
            
        return False
    
    def _extract_content(self, soup: BeautifulSoup, config: SiteConfig) -> List[str]:
        """æå–é¡µé¢å†…å®¹"""
        content_paragraphs = []
        
        # ç‰¹æ®Šå¤„ç†
        if hasattr(self, '_current_url') and 'huanqixiaoshuo.com' in self._current_url:
            # æŸ¥æ‰¾æ‰€æœ‰æ— classå’Œidå±æ€§çš„divï¼Œæ”¶é›†å…¶ä¸­çš„æ®µè½å†…å®¹
            all_divs = soup.find_all("div")
            for div in all_divs:
                # åªå¤„ç†æ— ç‰¹å®šclassæˆ–idçš„div
                if div.get('class') or div.get('id'):
                    continue
                
                # æŸ¥æ‰¾divä¸­çš„æ®µè½
                paragraphs = div.find_all('p')
                for p in paragraphs:
                    p_text = p.get_text(separator='\n', strip=True)
                    # è·³è¿‡å¯¼èˆªç›¸å…³çš„æ®µè½
                    if any(nav_text in p_text for nav_text in ['ä¸Šä¸€ç« ', 'ç›®å½•', 'ä¸‹ä¸€é¡µ', 'ä¸Šä¸€é¡µ', 'ä¸‹ä¸€ç« ']):
                        continue
                    # è·³è¿‡æç¤ºæ€§æ–‡å­—
                    if any(tip_text in p_text for tip_text in ['æœ¬ç« å°šæœªå®Œç»“', 'è¯·ç‚¹å‡»ä¸‹ä¸€é¡µ', 'æœ¬ç« å·²é˜…è¯»å®Œæ¯•', 'å…³é—­']):
                        continue
                    # è·³è¿‡å¤ªçŸ­çš„æ®µè½ï¼ˆå¯èƒ½æ˜¯å¯¼èˆªæˆ–å¹¿å‘Šï¼‰
                    if len(p_text) < 10:
                        continue
                    
                    content_paragraphs.append(p_text)
            
            return content_paragraphs
        
        # é€šç”¨å¤„ç†é€»è¾‘
        # å°è¯•ä¸åŒçš„å†…å®¹é€‰æ‹©å™¨
        for selector in config.content_selectors:
            try:
                if selector == 'div:not([id]):not([class])':
                    # æŸ¥æ‰¾æ— id/classçš„div
                    all_divs = soup.find_all("div")
                    for div in all_divs:
                        if div.get('class') or div.get('id'):
                            continue
                        paragraphs = div.find_all('p')
                        if len(paragraphs) > 3:  # å†…å®¹åŒºåŸŸé€šå¸¸æœ‰å¤šä¸ªæ®µè½
                            for p in paragraphs:
                                p_text = p.get_text(separator='\n', strip=True)
                                if p_text and not any(filter_text in p_text for filter_text in config.filters):
                                    content_paragraphs.append(p_text)
                            if content_paragraphs:
                                break
                else:
                    # ä½¿ç”¨CSSé€‰æ‹©å™¨
                    content_div = soup.select_one(selector)
                    if content_div:
                        # æå–æ–‡æœ¬æ®µè½
                        for elem in content_div.find_all(['p', 'div', 'br']):
                            text = elem.get_text(separator='\n', strip=True)
                            if text and not any(filter_text in text for filter_text in config.filters):
                                content_paragraphs.append(text)
                        if content_div.get_text(separator='\n', strip=True):
                            # å¦‚æœæ²¡æœ‰å­å…ƒç´ ï¼Œç›´æ¥è·å–æ–‡æœ¬
                            lines = content_div.get_text(separator='\n').split('\n')
                            for line in lines:
                                line = line.strip()
                                if line and not any(filter_text in line for filter_text in config.filters):
                                    content_paragraphs.append(line)
                
                if content_paragraphs:
                    break
                    
            except Exception:
                continue
        
        return content_paragraphs
    
    def _find_next_page(self, soup: BeautifulSoup, current_url: str, config: SiteConfig) -> Optional[str]:
        """æŸ¥æ‰¾ä¸‹ä¸€é¡µURL"""
        # ç‰¹æ®Šå¤„ç†ï¼šhuanqixiaoshuo.comç½‘ç«™ - ä½¿ç”¨è€ç‰ˆæœ¬å®Œå…¨ç›¸åŒçš„é€»è¾‘
        if 'huanqixiaoshuo.com' in current_url:
            # æ–¹æ³•1ï¼šä»é¡µé¢æ ‡é¢˜ä¸­è·å–é¡µç ä¿¡æ¯å¹¶æ„é€ ä¸‹ä¸€é¡µURL
            title_tag = soup.find('title')
            if title_tag:
                title_text = title_tag.string or ""
                # æŸ¥æ‰¾(å½“å‰é¡µ/æ€»é¡µæ•°)æ ¼å¼ï¼Œå¦‚(1/3)
                page_match = re.search(r'\((\d+)/(\d+)\)', title_text)
                if page_match:
                    current_page = int(page_match.group(1))
                    total_pages = int(page_match.group(2))
                    
                    safe_print(f"    é¡µé¢æ ‡é¢˜æ˜¾ç¤ºï¼šç¬¬{current_page}é¡µï¼Œå…±{total_pages}é¡µ")
                    
                    if current_page < total_pages:
                        # æ„é€ ä¸‹ä¸€é¡µURL
                        next_page = current_page + 1
                        # ä»current_urlä¸­å»æ‰.htmlåç¼€
                        base_url_no_ext = current_url.replace('.html', '')
                        next_page_url = f"{base_url_no_ext}_{next_page}.html"
                        safe_print(f"    æ„é€ ä¸‹ä¸€é¡µURL: {next_page_url}")
                        return next_page_url

            # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1å¤±è´¥ï¼ŒæŸ¥æ‰¾"ä¸‹ä¸€é¡µ"é“¾æ¥
            for link in soup.find_all("a", href=True):
                link_text = link.get_text(strip=True)
                if "ä¸‹ä¸€é¡µ" in link_text:
                    href = link.get("href")
                    if href.startswith("/"):
                        next_page_url = f"https://www.huanqixiaoshuo.com{href}"
                    else:
                        next_page_url = urllib.parse.urljoin(current_url, href)
                    return next_page_url
            
            return None
        
        # é€šç”¨å¤„ç†é€»è¾‘ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        # æ–¹æ³•1: ä»æ ‡é¢˜è·å–é¡µç ä¿¡æ¯
        title_tag = soup.find('title')
        if title_tag and config.page_info_pattern:
            title_text = title_tag.string or ""
            page_match = re.search(config.page_info_pattern, title_text)
            if page_match:
                current_page = int(page_match.group(1))
                total_pages = int(page_match.group(2))
                
                if current_page < total_pages:
                    # æ„é€ ä¸‹ä¸€é¡µURL
                    next_page = current_page + 1
                    for pattern in config.next_page_patterns:
                        try:
                            base_url_no_ext = re.sub(r'\.html$', '', current_url)
                            if '_' in pattern:
                                next_page_url = f"{base_url_no_ext}_{next_page}.html"
                            else:
                                next_page_url = f"{base_url_no_ext}/{next_page}.html"
                            return next_page_url
                        except:
                            continue
        
        # æ–¹æ³•2: æŸ¥æ‰¾"ä¸‹ä¸€é¡µ"é“¾æ¥
        next_links = soup.find_all('a', string=re.compile(r'ä¸‹ä¸€é¡µ|ä¸‹é¡µ|next', re.I))
        for link in next_links:
            href = link.get('href')
            if href:
                if href.startswith('http'):
                    return href
                elif href.startswith('/'):
                    parsed_url = urlparse(current_url)
                    return f"{parsed_url.scheme}://{parsed_url.netloc}{href}"
                else:
                    return urllib.parse.urljoin(current_url, href)
        
        return None
    
    def _clean_content(self, content_list: List[str], config: SiteConfig) -> List[str]:
        """æ¸…ç†å†…å®¹"""
        cleaned = []
        for line in content_list:
            # æ¸…ç†HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
            line = re.sub(r'<[^>]+>', '', line)
            line = re.sub(r'&nbsp;|&lt;|&gt;|&amp;', ' ', line)
            # åªå‹ç¼©ç©ºæ ¼å’Œåˆ¶è¡¨ç¬¦ï¼Œä¿ç•™æ¢è¡Œç¬¦
            line = re.sub(r'[ \t]+', ' ', line)
            line = line.strip()
            
            # è¿‡æ»¤æ— æ•ˆå†…å®¹
            if (line and 
                len(line) > 5 and  # å¤ªçŸ­çš„è¡Œ
                not any(filter_text in line for filter_text in config.filters) and
                not re.match(r'^[\d\s\-_]+$', line)):  # çº¯æ•°å­—æˆ–ç¬¦å·
                cleaned.append(line)
        
        return cleaned
    
    def parse_chapter_range(self, range_input: str, total_chapters: int) -> Tuple[int, int]:
        """è§£æç« èŠ‚èŒƒå›´è¾“å…¥"""
        if not range_input.strip():
            return 0, total_chapters
        
        range_input = range_input.strip()
        
        try:
            # æ ¼å¼1: "100-200" æˆ– "100:200"
            if '-' in range_input or ':' in range_input:
                separator = '-' if '-' in range_input else ':'
                parts = range_input.split(separator)
                
                if len(parts) == 2:
                    start_str, end_str = parts
                    
                    # å¤„ç†èµ·å§‹ä½ç½®
                    if start_str.strip():
                        start = int(start_str.strip())
                        start = max(1, start)  # æœ€å°ä¸º1
                    else:
                        start = 1
                    
                    # å¤„ç†ç»“æŸä½ç½®
                    if end_str.strip():
                        end = int(end_str.strip())
                        end = min(end, total_chapters)  # æœ€å¤§ä¸ºæ€»ç« èŠ‚æ•°
                    else:
                        end = total_chapters
                    
                    # è½¬æ¢ä¸º0åŸºç´¢å¼•
                    return start - 1, end
            
            # æ ¼å¼2: "100+" è¡¨ç¤ºä»ç¬¬100ç« å¼€å§‹
            elif range_input.endswith('+'):
                start = int(range_input[:-1].strip())
                start = max(1, start)
                return start - 1, total_chapters
            
            # æ ¼å¼3: å•ä¸ªæ•°å­—ï¼Œè¡¨ç¤ºåªä¸‹è½½è¿™ä¸€ç« 
            else:
                chapter_num = int(range_input)
                chapter_num = max(1, min(chapter_num, total_chapters))
                return chapter_num - 1, chapter_num
                
        except ValueError:
            safe_print(f"âŒ æ— æ³•è§£æç« èŠ‚èŒƒå›´: {range_input}")
            return 0, total_chapters
    
    def filter_chapters_by_range(self, chapters: List[ChapterInfo], range_input: str) -> List[ChapterInfo]:
        """æ ¹æ®ç”¨æˆ·è¾“å…¥è¿‡æ»¤ç« èŠ‚"""
        if not range_input.strip():
            return chapters
        
        total_chapters = len(chapters)
        start_idx, end_idx = self.parse_chapter_range(range_input, total_chapters)
        
        # éªŒè¯èŒƒå›´
        if start_idx >= end_idx or start_idx >= total_chapters:
            safe_print(f"âŒ æ— æ•ˆçš„ç« èŠ‚èŒƒå›´ï¼Œæ€»å…±æœ‰ {total_chapters} ç« ")
            return chapters
        
        filtered_chapters = chapters[start_idx:end_idx]
        safe_print(f"ğŸ“– é€‰æ‹©ç« èŠ‚èŒƒå›´: ç¬¬{start_idx + 1}-{end_idx}ç«  (å…±{len(filtered_chapters)}ç« )")
        
        return filtered_chapters
    
    def crawl_novel(self, catalog_url: str, max_workers: int = 3, chapters: List[ChapterInfo] = None, output_dir: str = None, auto_merge: bool = False):
        """çˆ¬å–å°è¯´å¹¶ä¿å­˜"""
        if not chapters:
            safe_print("âŒ æœªæä¾›ç« èŠ‚åˆ—è¡¨ï¼Œæ— æ³•ç»§ç»­ã€‚")
            return

        original_chapter_count = len(chapters)

        if not output_dir:
            parsed_url = urlparse(catalog_url)
            site_name = parsed_url.netloc.replace('www.', '').replace('.', '_')
            output_dir = f"novels_{site_name}"
        
        Path(output_dir).mkdir(exist_ok=True)
        safe_print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

        # --- æ–­ç‚¹ç»­ä¼ æ ¸å¿ƒé€»è¾‘ ---
        # å¦‚æœè·³è¿‡æ–‡ä»¶æ£€æŸ¥ï¼Œç›´æ¥ä¸‹è½½æ‰€æœ‰ç« èŠ‚
        if getattr(self, 'skip_check_files', False):
            safe_print("âš ï¸ å·²è·³è¿‡æ–‡ä»¶æ£€æŸ¥ï¼Œå°†é‡æ–°ä¸‹è½½æ‰€æœ‰ç« èŠ‚")
            chapters_to_download = chapters
            skipped_count = 0
        else:
            downloaded_titles = self.get_downloaded_chapters(output_dir)
            chapters_to_download = [
                ch for ch in chapters if self._sanitize_filename(ch.title) not in downloaded_titles
            ]
            skipped_count = original_chapter_count - len(chapters_to_download)

            if skipped_count > 0:
                safe_print(f"ğŸ”„ æ£€æµ‹åˆ° {skipped_count} ä¸ªå·²ä¸‹è½½ç« èŠ‚ï¼Œå°†è‡ªåŠ¨è·³è¿‡ã€‚")

            if not chapters_to_download:
                safe_print("âœ… æ‰€æœ‰ç« èŠ‚å‡å·²åœ¨æœ¬åœ°ï¼Œæ— éœ€ä¸‹è½½ã€‚")
                self._show_completion_stats(0, 0, skipped_count, output_dir)
                if auto_merge or self._ask_merge_chapters():
                    self.merge_chapters_to_txt(output_dir)
                return
        # --- æ–­ç‚¹ç»­ä¼ æ ¸å¿ƒé€»è¾‘ç»“æŸ ---

        if RICH_AVAILABLE:
            success_count = self._download_chapters_with_progress(
                chapters_to_download, 
                output_dir, 
                max_workers,
                total_chapters=original_chapter_count,
                initial_advance=skipped_count
            )
        else:
            success_count = self._download_chapters_simple(chapters_to_download, output_dir, max_workers)

        self._show_completion_stats(success_count, len(chapters_to_download), skipped_count, output_dir)

        if auto_merge or self._ask_merge_chapters():
            self.merge_chapters_to_txt(output_dir)

    def _should_continue_download(self) -> bool:
        """è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­ä¸‹è½½"""
        if RICH_AVAILABLE:
            from rich.prompt import Confirm
            return Confirm.ask("ğŸ¤” æ˜¯å¦ç»§ç»­ä¸‹è½½å‰©ä½™ç« èŠ‚ï¼Ÿ[bold green](y/n, é»˜è®¤y)[/bold green]: ", default=True)
        else:
            return input("ğŸ¤” æ˜¯å¦ç»§ç»­ä¸‹è½½å‰©ä½™ç« èŠ‚ï¼Ÿ(y/n, é»˜è®¤y): ").strip().lower() in ['', 'y', 'yes']
    
    def _ask_merge_chapters(self) -> bool:
        """è¯¢é—®æ˜¯å¦åˆå¹¶ç« èŠ‚"""
        if RICH_AVAILABLE and console:
            from rich.panel import Panel
            from rich import box
            from rich.prompt import Confirm
            
            merge_help_text = (
                "å°†æ‰€æœ‰ä¸‹è½½çš„ç« èŠ‚åˆå¹¶æˆä¸€ä¸ª .txt æ–‡ä»¶ã€‚\n"
                "â€¢ ç« èŠ‚æ ‡é¢˜å°†è‡ªåŠ¨æ ¼å¼åŒ–\n"
                "â€¢ æ­£æ–‡å†…å®¹å°†è‡ªåŠ¨ç¼©è¿›\n"
                "â€¢ åˆ†é¡µæ ‡è®°å’Œæ— å…³å†…å®¹å°†è¢«ç§»é™¤"
            )
            
            panel = Panel(
                merge_help_text,
                title="[bold cyan]ğŸ“š ç« èŠ‚åˆå¹¶åŠŸèƒ½[/bold cyan]",
                border_style="cyan",
                expand=False
            )
            console.print(panel)
            
            return Confirm.ask("ğŸ“– [bold green]æ˜¯å¦æ‰§è¡Œåˆå¹¶ï¼Ÿ[/bold green]", default=True)
        else:
            print("\nğŸ“š ç« èŠ‚åˆå¹¶åŠŸèƒ½:")
            print("  â€¢ å°†æ‰€æœ‰.mdç« èŠ‚æ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ª.txtæ–‡ä»¶")
            return input("ğŸ¤” æ˜¯å¦åˆå¹¶æ‰€æœ‰ç« èŠ‚ä¸ºä¸€ä¸ªtxtæ–‡ä»¶ï¼Ÿ(y/n, é»˜è®¤n): ").strip().lower() in ['y', 'yes']
    
    def _download_chapters_with_progress(self, chapters: List[ChapterInfo], output_dir: str, max_workers: int, total_chapters: int, initial_advance: int) -> int:
        """ä½¿ç”¨richè¿›åº¦æ¡ä¸‹è½½ç« èŠ‚"""
        progress = Progress(
            TextColumn("[bold blue]ä¸‹è½½è¿›åº¦", justify="right"),
            BarColumn(bar_width=None),
            "[progress.percentage]{task.percentage:>3.1f}%",
            "â€¢",
            TextColumn("[green]{task.completed} of {task.total}"),
            "â€¢",
            TimeElapsedColumn(),
            "â€¢",
            TimeRemainingColumn(),
            transient=False,  # Keep progress bar after completion
            console=console
        )

        success_count = 0
        error_messages = []  # æ”¶é›†é”™è¯¯ä¿¡æ¯ï¼Œæœ€åç»Ÿä¸€æ˜¾ç¤º
        
        with progress:
            task = progress.add_task("ä¸‹è½½ä¸­...", total=total_chapters)
            progress.advance(task, advance=initial_advance)

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_chapter = {
                    executor.submit(self.crawl_single_chapter, chapter, output_dir, silent=True): chapter
                    for chapter in chapters
                }

                for future in as_completed(future_to_chapter):
                    chapter = future_to_chapter[future]
                    try:
                        result = future.result()
                        if result and result != "skipped":
                            success_count += 1
                    except Exception as e:
                        error_messages.append(f"âŒ ä¸‹è½½ç« èŠ‚ '{chapter.title}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    finally:
                        progress.advance(task, advance=1)
        
        # ä¸‹è½½å®Œæˆåæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
        if error_messages:
            safe_print("\n" + "\n".join(error_messages[:5]))  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
            if len(error_messages) > 5:
                safe_print(f"... è¿˜æœ‰ {len(error_messages) - 5} ä¸ªé”™è¯¯æœªæ˜¾ç¤º")
                
        return success_count

    def _download_chapters_simple(self, chapters: List[ChapterInfo], output_dir: str, max_workers: int) -> int:
        """ä¸ä½¿ç”¨richè¿›åº¦æ¡çš„ç®€å•ä¸‹è½½æ¨¡å¼"""
        success_count = 0
        total_count = len(chapters)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_chapter = {
                executor.submit(self.crawl_single_chapter, chapter, output_dir): chapter
                for chapter in chapters
            }

            for i, future in enumerate(as_completed(future_to_chapter)):
                chapter = future_to_chapter[future]
                try:
                    result = future.result()
                    if result and result != "skipped":
                        success_count += 1
                    safe_print(f"[{i + 1}/{total_count}] {'âœ…' if result else 'âŒ'} ä¸‹è½½: {chapter.title}")
                except Exception as e:
                    safe_print(f"[{i + 1}/{total_count}] âŒ ä¸‹è½½ç« èŠ‚ '{chapter.title}' å¤±è´¥: {e}")
        
        return success_count

    def _show_completion_stats(self, success_count: int, total_count: int, skipped_count: int, output_dir: str):
        """æ˜¾ç¤ºä¸‹è½½å®Œæˆåçš„ç»Ÿè®¡ä¿¡æ¯"""
        failure_count = total_count - success_count
        
        if not RICH_AVAILABLE:
            safe_print("\n" + "="*20)
            safe_print("ä¸‹è½½å®Œæˆ!")
            safe_print(f"æˆåŠŸ: {success_count}")
            if failure_count > 0:
                safe_print(f"å¤±è´¥: {failure_count}")
            if skipped_count > 0:
                safe_print(f"è·³è¿‡: {skipped_count}")
            safe_print(f"æ€»è®¡: {success_count + failure_count + skipped_count}")
            safe_print(f"æ–‡ä»¶ä¿å­˜åœ¨: {output_dir}")
            safe_print("="*20)
            return
            
        from rich.table import Table
        from rich.panel import Panel

        stats_table = Table(title="ğŸ“Š ä¸‹è½½ç»Ÿè®¡", show_header=False, box=None)
        stats_table.add_column(style="green")
        stats_table.add_column(style="bold magenta")
        
        stats_table.add_row("âœ… æˆåŠŸä¸‹è½½:", f"{success_count} ç« ")
        if failure_count > 0:
            stats_table.add_row("âŒ ä¸‹è½½å¤±è´¥:", f"[red]{failure_count} ç« [/red]")
        if skipped_count > 0:
            stats_table.add_row("ğŸ”„ å·²è·³è¿‡:", f"{skipped_count} ç« ")

        stats_table.add_row("ğŸ’¾ ä¿å­˜ä½ç½®:", f"[cyan]{output_dir}[/cyan]")
        
        total_chapters_in_dir = len(self.get_downloaded_chapters(output_dir))
        stats_table.add_row("ğŸ“ ç›®å½•æ€»æ•°:", f"{total_chapters_in_dir} ç« ")

        panel = Panel(
            stats_table,
            title="ğŸ‰ [bold green]ä¸‹è½½å®Œæˆ[/bold green] ğŸ‰",
            expand=False,
            border_style="green"
        )
        console.print(panel)

    def filter_valid_chapters(self, chapters: List[ChapterInfo]) -> List[ChapterInfo]:
        """è¿‡æ»¤æœ‰æ•ˆçš„ç« èŠ‚é“¾æ¥."""
        filtered_chapters = []

        exclude_keywords = [
            'ä¹¦æ¶', 'æ¨è', 'æ’è¡Œ', 'ä¹¦å•', 'ç™»å½•', 'æ³¨å†Œ', 'å……å€¼',
            'ç­¾åˆ°', 'ä½œè€…', 'ç®€ä»‹', 'ç›®å½•', 'è®¾ç½®', 'å…³äº'
        ]
        
        chapter_patterns = [
            r'ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+\s*[ç« ç« èŠ‚å·]',
            r'^\d+$' # çº¯æ•°å­—æ ‡é¢˜
        ]

        for chapter in chapters:
            title = chapter.title.strip()
            
            if not title or any(keyword in title for keyword in exclude_keywords):
                continue
            
            if len(title) > 50: # æ ‡é¢˜è¿‡é•¿
                continue

            # å¿…é¡»åŒ…å«ç« èŠ‚æ ‡è¯†æˆ–ä¸ºçŸ­æ•°å­—æ ‡é¢˜
            if any(re.search(p, title) for p in chapter_patterns) or len(title) < 5:
                 filtered_chapters.append(chapter)

        return filtered_chapters
        
    def extract_chapter_number(self, filename: str):
        """ä»æ–‡ä»¶åä¸­æå–ç« èŠ‚å·ç”¨äºæ’åº"""
        # åŒ¹é… "ç¬¬123ç« ", "ç« 123", "123"
        matches = re.findall(r'(\d+)', filename)
        if matches:
            return int(matches[0])
        return 99999 # æ— æ³•è§£æçš„æ’åœ¨åé¢

    def _normalize_title(self, raw_title: str) -> str:
        """æ ‡å‡†åŒ–ç« èŠ‚æ ‡é¢˜ä¸ºã€ç¬¬xxç«  ç« èŠ‚åã€æ ¼å¼"""
        raw_title = raw_title.strip()
        m = re.match(r'(ç¬¬[\u4e00-\u9fa5\d]+[ç« èŠ‚å·])\s*[:ï¼š_-]*\s*(.*)', raw_title)
        if m:
            number_part = m.group(1)
            name_part = m.group(2).strip()
            return f"{number_part} {name_part}" if name_part else number_part
        # è‹¥æ— æ³•è§£æåˆ™ç›´æ¥è¿”å›åŸæ ‡é¢˜
        return raw_title

    def clean_merge_content(self, content: str) -> str:
        """æ¸…ç†ç”¨äºåˆå¹¶çš„å•ç« å†…å®¹ï¼Œä¿ç•™æ®µè½ç©ºè¡Œï¼Œå¹¶è§„èŒƒæ ‡é¢˜æ ¼å¼"""
        # æ‹†åˆ†è¡Œï¼Œä¿ç•™ç©ºè¡Œä¿¡æ¯
        lines = content.split('\n')

        # å¤„ç†æ ‡é¢˜
        title = ""
        if lines and lines[0].startswith('# '):
            title = self._normalize_title(lines[0][2:].strip())
            lines = lines[1:]  # å»æ‰æ ‡é¢˜è¡Œ

        output_lines = []
        if title:
            output_lines.append(title)
            output_lines.append("")  # æ ‡é¢˜åç©ºè¡Œ

        previous_blank = False
        for raw in lines:
            line = raw.rstrip()  # ä¿ç•™æ®µå†…ç©ºæ ¼å·¦ä¾§å‰”é™¤å³ä¾§\n
            if not line.strip():
                # ç©ºè¡Œ: ä¿è¯æ®µè½ä¹‹é—´åªæœ‰ä¸€ä¸ªç©ºè¡Œ
                if not previous_blank and output_lines:
                    output_lines.append("")
                    previous_blank = True
                continue

            previous_blank = False
            output_lines.append(line.strip())

        return '\n'.join(output_lines)

    def merge_chapters_to_txt(self, output_dir: str) -> bool:
        """å°†æ‰€æœ‰ç« èŠ‚æ–‡ä»¶åˆå¹¶æˆä¸€ä¸ªtxtæ–‡ä»¶"""
        chapters_dir = Path(output_dir)
        novel_name = getattr(self, 'novel_title', None)
        if not novel_name:
            novel_name = chapters_dir.name.replace('novels_', '')

        # ç”Ÿæˆå®‰å…¨æ–‡ä»¶å
        safe_novel_name = self._sanitize_filename(novel_name)
        output_file = chapters_dir.parent / f"{safe_novel_name}.txt"
        
        md_files = sorted(list(chapters_dir.glob("*.md")), key=lambda f: self.extract_chapter_number(f.name))

        if not md_files:
            safe_print(f"âŒ åœ¨ '{output_dir}' ä¸­æœªæ‰¾åˆ°ç« èŠ‚æ–‡ä»¶ã€‚", style="bold red")
            return False

        safe_print(f"ğŸ”„ å¼€å§‹åˆå¹¶ {len(md_files)} ä¸ªç« èŠ‚åˆ° '{output_file.name}'...")
        
        with open(output_file, 'w', encoding='utf-8') as outfile:
            for md_file in md_files:
                try:
                    with open(md_file, 'r', encoding='utf-8') as infile:
                        content = infile.read()
                        cleaned_content = self.clean_merge_content(content)
                        outfile.write(cleaned_content)
                        outfile.write('\n\n')
                except Exception as e:
                    safe_print(f"âš ï¸ è¯»å–æˆ–å¤„ç†æ–‡ä»¶ '{md_file.name}' å¤±è´¥: {e}", style="yellow")
        
        safe_print(f"âœ… åˆå¹¶å®Œæˆï¼", style="bold green")
        return True

    def _extract_novel_title(self, soup: BeautifulSoup, catalog_url: str) -> Optional[str]:
        """å°è¯•ä»ç›®å½•é¡µæå–å°è¯´æ ‡é¢˜

        ä¼˜å…ˆçº§:
        1. OpenGraph å…ƒæ•°æ® og:novel:book_name
        2. å¸¸è§çš„ <h1> æ ‡ç­¾ (id æˆ– class å«æœ‰ title / book / name)
        3. <title> æ ‡ç­¾ â€” å»æ‰ç½‘ç«™åç­‰å¤šä½™ä¿¡æ¯
        """
        # 1) og:novel:book_name
        meta_book = soup.find('meta', attrs={'property': 'og:novel:book_name'})
        if meta_book and meta_book.get('content'):
            return meta_book.get('content').strip()

        # 2) å¸¸è§çš„ <h1> æ ‡ç­¾
        h1_candidates = soup.find_all('h1')
        for h1 in h1_candidates:
            h1_text = h1.get_text(strip=True)
            if 0 < len(h1_text) < 50:  # ç®€å•é™å®šé•¿åº¦
                if any(keyword in h1_text for keyword in ['æœ€æ–°ç« èŠ‚', 'ç« èŠ‚', '>>']):
                    # è¿‡æ»¤å¯èƒ½çš„è¯´æ˜æ€§æ–‡å­—
                    continue
                return h1_text

        # 3) <title> æ ‡é¢˜ â€” å»æ‰åˆ†éš”ç¬¦åçš„ç«™ç‚¹ä¿¡æ¯
        title_tag = soup.find('title')
        if title_tag and title_tag.string:
            raw_title = title_tag.string.strip()
            # å¸¸è§åˆ†éš”ç¬¦
            for sep in ['_', '-', '|', 'ï¼', 'â€”']:
                if sep in raw_title:
                    raw_title = raw_title.split(sep)[0].strip()
                    break
            if raw_title:
                return raw_title

        return None

   